{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Ch06.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/nknu-class/blob/main/NLP/notebooks/Ch06-RNN-TF1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doCoqaOBvLx4"
      },
      "source": [
        "# 6-1 文字資料處理"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgodqumivLx_"
      },
      "source": [
        "### 程式 6.1 單字的 one-hot encoding (簡易版)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSrdY5_bvLyA"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']  # 初始資料：每一個樣本是一個輸入項目(在這範例中，樣本是一個句子，但也可以是整個文件)\n",
        "\n",
        "token_index = {}  # 建立資料中所有 tokens 的索引\n",
        "for sample in samples:\n",
        "    for word in sample.split():  # 透過 split()方法對樣本進行分詞。在真實案例中，還要移除樣本中的標點符號與特殊字元\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1  # 為每個文字指定一個唯一索引。請注意，不要把索引 0 指定給任何文字\n",
        "\n",
        "max_length = 10  # 將樣本向量化。每次只專注處理每個樣本中的第一個 max_length 文字\n",
        "\n",
        "results = np.zeros(shape=(len(samples),  # 用來儲存結果的 Numpy array\n",
        "                          max_length,\n",
        "                          max(token_index.values()) + 1))  \n",
        "print(results.shape) # shape=(2, 10, 11), 共 2 個樣本, 每個樣本只看前 10 個文字, 總樣本共有 10 個 token, 索引號到 11, 因為 0 不用。\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1.\n",
        "print(token_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr08mmTlvLyC"
      },
      "source": [
        "### 程式 6.2 字元的 one-hot encoding (簡易版)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUKVE5nhvLyD"
      },
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable  # 所有可印出的 ASCII 字元的字串, '0123456789abc....'\n",
        "print(len(characters))\n",
        "\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "print(results.shape) \n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "\tfor j, character in enumerate(sample):\n",
        "\t\tindex = token_index.get(character)\n",
        "\t\tresults[i, j, index] = 1.\n",
        "print(results[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig22RLZjvLyE"
      },
      "source": [
        "### 程式 6.3 用 Keras 做文字的 one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNFB4IcEvLyE"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer  # 匯入 Keras 分詞器\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']  # 初始資料\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000) # 1. 建立一個分詞器, 設定上僅考慮 1, 000 個最常用的文字 (token), 也就是只會看初始資料的前 1000 個文字\n",
        "tokenizer.fit_on_texts(samples)  # 建立文字對應的索引值, 一樣依出現順序來決定, 0 一樣不使用\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(samples)  # 將初始資料中的文字轉換成對應的索引值 list\n",
        "print(sequences) # [[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]\n",
        "\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')  # 可以直接取得 one-hot 的二進位表示方式。此分詞 tokenizer 支援除了 one-hot 編碼以外, 也有支援其他的向量化方法\n",
        "print(one_hot_results.shape) # (2, 1000) 共 2 個樣本, 每個樣本中的文字對應到的 token 位置 (1000個)\n",
        "word_index = tokenizer.word_index  # 計算完成後, 取得文字與索引間的對應關聯 \n",
        "print(word_index) # {'the': 1, 'cat': 2, 'sat': 3, ... 'my': 8, 'homework': 9}\n",
        "print('找到 %s 個唯一的 tokens.' % len(word_index)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVqIGCEvLyF"
      },
      "source": [
        "### 程式 6.4 使用雜湊技巧的單字 one-hot encoding (簡易版本)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuzfqH5AvLyG"
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "dimensionality = 1000 #  將文字儲存成大小為 1, 000 的向量。如果有接近 1, 000 個文字(或更多), 將會造成許多雜湊碰撞, 這會降低此編碼方法的準確性\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "\tfor j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "\t\tindex = abs(hash(word)) % dimensionality # ← 將文字雜湊成 0 到 1, 000 之間的隨機整數索引\n",
        "\t\tresults[i, j, index] = 1.\n",
        "print(results.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tksNI0yBvLyH"
      },
      "source": [
        "### 使用嵌入向量層學習文字嵌入向量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAqUpc5zvLyI"
      },
      "source": [
        "### 程式 6.5 建立一個嵌入層 (Embedding Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wTPQY4tavLyI"
      },
      "source": [
        "from keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000, 64)  # 建立嵌入向量層至少須指定兩個參數：可能的 tokens 數量 (此處為 1, 000, 最少是 1+最大文字索引) 和嵌入向量的維數 (此處為 64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4tyT3ZvvLyJ"
      },
      "source": [
        "### 程式 6.6 載入 IMDB, 整理成適合供 Embedding 層使用的資料\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFhR8ijUvLyJ"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000  # ←設定作為特徵的文字數量\n",
        "maxlen = 20 # ←在 20 個文字之後切掉文字資料 (在 max_features 最常見的文字中)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)  # ←將資料以整數 lists 載入\n",
        "print(x_train.shape)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)  # ←將整數 lists 轉換為 2D 整數張量, 形狀為(樣本數 samples, 最大長度 maxlen)\n",
        "print(x_train.shape)\n",
        "print(x_train[0])\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gub8z7uIvLyJ"
      },
      "source": [
        "### 程式 6.7 把 IMDB 資料提供給 Embedding layer和分類器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr8WrEq9vLyK"
      },
      "source": [
        "# 1. 指定嵌入向量層的最大輸入長度, 以便之後可以攤平嵌入向量的輸入。在嵌入向量層之後, 啟動函數輸出的 shape 為 (樣本數 samples, 最大長度 maxlen, 8）\n",
        "# 2. 將嵌入向量的 3D 張量展平為 2D 張量, 形狀為(樣本數 samples, 最大長度 maxlen * 8)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen)) # ←1...\n",
        "\n",
        "model.add(Flatten()) # ← 2...\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid')) # ← 在頂部加上分類器\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, \n",
        "                    y_train,epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpkA_cZmvLyK"
      },
      "source": [
        "### 使用預先訓練的文字嵌入向量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13PTyulIvLyK"
      },
      "source": [
        "### 程式 6.8 處理原始 IMDB 資料的標籤"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I00IKS9evLyL"
      },
      "source": [
        "import os\n",
        "\n",
        "imdb_dir = r'C:\\Users\\Admin\\Desktop\\Google\\Python Keras\\ch06\\aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding = 'utf8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXJ2fLpVvLyL"
      },
      "source": [
        "print(len(texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdu0E6PxvLyL"
      },
      "source": [
        "### 程式 6.9 對原始 IMDB 資料的文字資料進行向量化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9DUDscyvLyM"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 100  # 100 個文字後切斷評論 (只看評論的前 100 個字)\n",
        "training_samples = 200  # 以 200 個樣本進行訓練\n",
        "validation_samples = 10000 # 以 10, 000 個樣本進行驗證\n",
        "max_words = 10000  # 僅考慮資料集中的前 10, 000 個單詞\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts) # 將文字轉成整數 list 的序列資料\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index[: 10])\n",
        "print('共使用了 %s 個 token 字詞.' % len(word_index))\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen) # 只取每個評論的前 100 個字 (多切少補) 作為資料張量\n",
        "labels = np.asarray(labels)  # 將標籤 list 轉為 Numpy array (標籤張量)\n",
        "\n",
        "print('資料張量 shape:', data.shape) # (25000, 100)\n",
        "print('標籤張量 shape:', labels.shape) # (25000,)\n",
        "\n",
        "indices = np.arange(data.shape[0])  # 將資料拆分為訓練集和驗證集, 但首先要將資料打散, 因為所處理的資料是有順序性的樣本資料 (負評在前, 然後才是正評)\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeNaZLx0vLyM"
      },
      "source": [
        "### 程式 6.10 解析 GloVe 文字嵌入向量檔案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQjtguU_vLyM"
      },
      "source": [
        "glove_dir = r'C:\\Users\\Admin\\Desktop\\Google\\Python Keras\\ch06\\glove.6B'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='UTF-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('共有 %s 個文字嵌入向量' % len(embeddings_index))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSu8r9lEvLyN"
      },
      "source": [
        "### 程式 6.11 準備 GloVe 文字嵌入向量矩陣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtMl5rRFvLyN"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector  # ←嵌入向量索引中找不到的文字將為 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72sUvRqsvLyN"
      },
      "source": [
        "### 程式 6.12 模型定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qR0m-WQvLyN"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# 參數 樣本數, 嵌入向量維度, \n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY8D2IQcvLyO"
      },
      "source": [
        "###  將預訓練的文字嵌入向量載入到嵌入向量層中"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yRLPH6uCvLyO"
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNAY0o3vLyO"
      },
      "source": [
        "### 程式 6.13 訓練和驗證"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c7H-IJfvLyO"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NScYaGfovLyO"
      },
      "source": [
        "### 程式 6.14 繪製結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0EdEBWJvLyP"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLw8fCfEvLyP"
      },
      "source": [
        "### 程式 6.15 訓練相同模型而不使用預先訓練的文字嵌入向量"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBebYLApvLyP"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "\n",
        "############### 繪製\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icp00VBmvLyP"
      },
      "source": [
        "### 程式 6.16 對測試資料進行分詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Y3uu6p0QvLyQ"
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "\tdir_name = os.path.join(test_dir, label_type)\n",
        "\tfor fname in sorted(os.listdir(dir_name)):\n",
        "\t\tif fname[-4:] == '.txt':\n",
        "\t\t\tf = open(os.path.join(dir_name, fname), encoding='UTF-8')\n",
        "\t\t\ttexts.append(f.read())\n",
        "\t\t\tf.close()\n",
        "\t\t\tif label_type == 'neg':\n",
        "\t\t\t\tlabels.append(0)\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXGDc9prvLyQ"
      },
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2ofyH4iVvLyQ"
      },
      "source": [
        "# 6-2 了解遞歸神經網路"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "zDXVJqAIvLyQ"
      },
      "source": [
        "### 程式 6.17 以 Numpy 實現簡單的 RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WXpmZrhIvLyQ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 100   # 輸入序列資料中的時間點數量\n",
        "input_features = 32  # 輸入特徵空間的維度數\n",
        "output_features = 64  # 輸出特徵空間的維度數\n",
        "\n",
        "inputs = np.random.random((timesteps, input_features))  # 輸入資料：隨機產生數值以便示範\n",
        "\n",
        "state_t = np.zeros((output_features, ))  # 初始狀態：全零向量\n",
        "\n",
        "W = np.random.random((output_features, input_features))  # 建立隨機權重矩陣\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features, ))\n",
        "\n",
        "successive_outputs = []\n",
        "for input_t in inputs:  #  input_t 是個向量, shape 為 (input_features, )\n",
        "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)  # 結合輸入與當前狀態(前一個輸出)以取得當前輸出\n",
        "    successive_outputs.append(output_t)  # 將此輸出儲存在列表中\n",
        "    state_t = output_t  #更新下一個時間點的網絡狀態\n",
        "\n",
        "final_output_sequence = np.concatenate(successive_outputs, axis=0)  \n",
        "print(final_output_sequence.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BVFdB4s5vLyR"
      },
      "source": [
        "### 程式 6.18 準備 IMDB 資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGs_MeZxvLyR"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000  #考慮做為特徵的文字數量\n",
        "maxlen = 500  # 我們只看每篇評論的前 500 個文字\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "print('讀取資料...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences') # 25000 筆訓練用序列資料 (評論)\n",
        "print(len(input_test), 'test sequences')\t# 25000 筆測試用序列資料\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen) # 1. 只看每篇評論的前 500 個文字, 多的去除, 不足填補\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\t# shape=(25000, 500)\n",
        "print('input_test shape:', input_test.shape)\t# shape=(25000, 500)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Gon5HxUDvLyR"
      },
      "source": [
        "### 程式 6.19 以嵌入向量 Embedding 層和 SimpleRNN 層訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rvjn1F8vLyR"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "CFMBLX33vLyR"
      },
      "source": [
        "### 程式 6.20 繪製結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO4E_5DWvLyS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "iNceZ-nwvLyS"
      },
      "source": [
        "### 程式 6.21 在 Keras 中使用 LSTM 層"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "AkXuGG02vLyS"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "history = model.fit(input_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQs9KXBVvLyS"
      },
      "source": [
        "# 6-3 遞歸神經網路的進階使用方法\n",
        "### 程式 6.22 檢視耶拿天氣資料集的資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uvV8GCJvLyS"
      },
      "source": [
        "import os\n",
        "\n",
        "data_dir = r'C:\\Users\\Admin\\Desktop\\Google\\Python Keras\\ch06\\jena_climate\\jena_climate_2009_2016.csv'  # 您的 jena_climate 資料夾路徑\n",
        "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv') # 資料集完整路徑\n",
        "\n",
        "f = open(fname)\n",
        "data = f.read()\n",
        "f.close()\n",
        "\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "print(header)  # \n",
        "print(len(header))\n",
        "print(len(lines))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74fg5cxfvLyT"
      },
      "source": [
        "### 程式 6.23 解析資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wJG7ip1vLyT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(',')[1:]]\n",
        "    float_data[i, :] = values\n",
        "print(float_data.shape)   # 共有 420551 個時間點的天氣資料, 每個包含 14 種天氣數值"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmuTArNLvLyT"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "temp = float_data[:, 1] # 索引 1 為 temperature 資料\n",
        "plt.plot(range(len(temp)), temp)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1q7nQVOvLyU"
      },
      "source": [
        "plt.plot(range(1440), temp[:1440])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ4YoHfmvLyU"
      },
      "source": [
        "### 程式 6.24 標準化資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "K8nLc8TGvLyU"
      },
      "source": [
        "mean = float_data[:200000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:200000].std(axis=0)\n",
        "float_data /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoSbO0NgvLyU"
      },
      "source": [
        "### 程式 6.25 定義產生器函式以產生時間序列樣本資料及其目標資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gErksXsIvLyU"
      },
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "        \n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "\n",
        "        samples = np.zeros((len(rows),\n",
        "                        lookback // step,\n",
        "                        data.shape[-1]))\n",
        "        targets = np.zeros((len(rows), ))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ra2B439pvLyV"
      },
      "source": [
        "### 程式 6.26 建立訓練資料、驗證資料和測試資料產生器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PSJ-KwcuvLyV"
      },
      "source": [
        "lookback = 1440\n",
        "step = 6\n",
        "delay = 144\n",
        "batch_size = 128\n",
        "\n",
        "# 訓練資料產生器\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step,\n",
        "                      batch_size=batch_size)\n",
        "# 驗證資料產生器\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "# 測試資料產生器\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) // batch_size  #← 1...\n",
        "test_steps = (len(float_data) - 300001 - lookback) // batch_size  #←2..\n",
        "\n",
        "#1. val_gen 產生器需要運行多少次才可以產生完整的驗證集\n",
        "#2. test_gen 產生器需要運行多少次才可以產生完整的測試集\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "SyyUkhSNvLyV"
      },
      "source": [
        "### 程式 6.27 計算一般常識性基準方法 MAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1KdfHtkvLyV"
      },
      "source": [
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):   # ← 計算所有的驗證集資料\n",
        "        samples, targets = next(val_gen) # ← 驅動產生器, 請見下面的小編補充\n",
        "        print(samples.shape) # shape=(128, 240, 14), 因為回朔為 1440 個時間點, 並以 6 個時間點為間隔進行取樣,所以共產生 1440/6=240 個時間點資料\n",
        "        print(targets.shape)  # shape=(128,) 128 筆溫度答案 \n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "        print(np.mean(batch_maes))  #　2.56488743498\n",
        "\n",
        "evaluate_naive_method()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "i-hCI5y0vLyV"
      },
      "source": [
        "### 程式 6.28 訓練和評估密集連接模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nIeKzfgvLyW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "rJdwAl8FvLyW"
      },
      "source": [
        "### 程式 6.29 繪製結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYmzifEcvLyW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "RgFDj3qwvLyW"
      },
      "source": [
        "### 程式 6.30 訓練和驗證 GRU 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBLAECEYvLyW"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLG4eh1WvLyX"
      },
      "source": [
        "### 程式 6.31 訓練和驗證使用丟棄法常規化的 GRU 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8BjOqWRQvLyX"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewkLEKcCvLyX"
      },
      "source": [
        "### 程式 6.23 訓練和驗證一個使用丟棄法的堆疊 GRU 模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5oUWtYzCvLyX"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5,\n",
        "                     return_sequences=True,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "\n",
        "model.add(layers.GRU(64, activation='relu',\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5))\n",
        "\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqFFL3_dvLyX"
      },
      "source": [
        "### 程式 6.33 使用反向序列資料訓練和驗證 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOpcEcIQvLyX"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 10000  # ←考慮作為特徵的文字數量\n",
        "maxlen = 500  #← 只看每篇評論的前 500 個字\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)  # ←載入資料\n",
        "\n",
        "\n",
        "x_train = [x[::-1] for x in x_train]    # ←將訓練資料進行反向順序排列\n",
        "x_test = [x[::-1] for x in x_test]      # ←將測試資料進行反向順序排列\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)  # ←填補序列資料\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cxzpdq6vLyY"
      },
      "source": [
        "### 程式 6.34 訓練和驗證雙向 LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqcJdb3tvLyY"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvYfEt9vvLyY"
      },
      "source": [
        "### 程式 6.35 訓練雙向 GRU 進行溫度預測任務"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "_m1f0FQEvLyY"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Bidirectional(layers.GRU(32), \n",
        "                               input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HGDJX0BKvLyY"
      },
      "source": [
        "### 程式 6.36 準備 IMDB 資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J7-ugKCvLyY"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "UhRVGndBvLyZ"
      },
      "source": [
        "### 程式 6.37 以 IMDB 資料訓練和驗證簡單的 1D 卷積神經網路"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuNJXpQpvLyZ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HK9hj3qxvLyZ"
      },
      "source": [
        "### 程式 6.38 以耶拿資料訓練和驗證一個簡單的 1D 卷積神經網路"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxJESrt4vLyZ"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "                        input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')\n",
        "\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRg4zkUKvLyZ"
      },
      "source": [
        "### 程式 6.39 為耶拿數據集準備高解析率的資料產生器"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3dpzb_ZjvLyZ"
      },
      "source": [
        "step = 3  # 先前設定為 6(每小時 1 個時間點), 現在設定為 3(每 30 分鐘 1 個時間點)\n",
        "lookback = 720\n",
        "delay = 144\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay,\n",
        "                      min_index=0,\n",
        "                      max_index=200000,\n",
        "                      shuffle=True,\n",
        "                      step=step)\n",
        "\n",
        "val_gen = generator(float_data,\n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=200001,\n",
        "                    max_index=300000,\n",
        "                    step=step)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=300001,\n",
        "                     max_index=None,\n",
        "                     step=step)\n",
        "\n",
        "val_steps = (300000 - 200001 - lookback) // 128\n",
        "test_steps = (len(float_data) - 300001 - lookback) // 128\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01D3dqeXvLya"
      },
      "source": [
        "### 程式 6.40 結合 1D 卷積層和 GRU 層的模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZdnBMpRvLya"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Conv1D(32, 5, activation='relu',\n",
        "                        input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.MaxPooling1D(3))\n",
        "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
        "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer=RMSprop(), loss='mae')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "F-N9_NrVvLya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aXNF0mngvLya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OtKOYt53vLya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dBTF0pecvLya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0XucO633vLya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}