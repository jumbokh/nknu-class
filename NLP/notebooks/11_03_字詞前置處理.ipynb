{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "11_03_字詞前置處理.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/nknu-class/blob/main/NLP/notebooks/11_03_%E5%AD%97%E8%A9%9E%E5%89%8D%E7%BD%AE%E8%99%95%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjRl3Ei2Srcc"
      },
      "source": [
        "# 字詞前置處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ52BD3fSrce"
      },
      "source": [
        "# 載入相關套件\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hqc92iASrcg"
      },
      "source": [
        "# 測試文章段落\n",
        "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
        "     \" And yesterday was the best day ever.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEdagA80Srcg"
      },
      "source": [
        "## 分割字句"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7UlfKJIWSrcg",
        "outputId": "63824a9f-e995-40cd-d0e8-8e327365c2b1"
      },
      "source": [
        "# 分割字句\n",
        "nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Today is a great day.',\n",
              " 'It is even better than yesterday.',\n",
              " 'And yesterday was the best day ever.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOs8Q3BaSrch"
      },
      "source": [
        "## 分詞"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7cyCDiSrci",
        "outputId": "bb842cbe-ff39-4a44-b27b-a84ed405e776"
      },
      "source": [
        "# 分詞\n",
        "nltk.word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Today',\n",
              " 'is',\n",
              " 'a',\n",
              " 'great',\n",
              " 'day',\n",
              " '.',\n",
              " 'It',\n",
              " 'is',\n",
              " 'even',\n",
              " 'better',\n",
              " 'than',\n",
              " 'yesterday',\n",
              " '.',\n",
              " 'And',\n",
              " 'yesterday',\n",
              " 'was',\n",
              " 'the',\n",
              " 'best',\n",
              " 'day',\n",
              " 'ever',\n",
              " '.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81pQ7cGgSrci"
      },
      "source": [
        "## 詞形還原"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--ZWjqAhSrci",
        "outputId": "c64099ab-6259-40cb-ef00-4050b4b38c0e"
      },
      "source": [
        "# 字根詞形還原(Stemming)\n",
        "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
        "ps = nltk.porter.PorterStemmer()\n",
        "' '.join([ps.stem(word) for word in text.split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'my system keep crash hi crash yesterday, our crash daili'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jApaLGQ3Srcj",
        "outputId": "1e92b1df-9de8-41b5-b982-78e10731edfe"
      },
      "source": [
        "# 依字典規則的詞形還原(Lemmatization)\n",
        "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
        "lem = nltk.WordNetLemmatizer()\n",
        "' '.join([lem.lemmatize(word) for word in text.split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'My system keep crashing his crashed yesterday, ours crash daily'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKGu2Yi2Srcj"
      },
      "source": [
        "## 停用詞(Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NFBdH49Srcj",
        "outputId": "ef8ebf31-6902-4f0f-c1b8-ecf9b242c351"
      },
      "source": [
        "# 標點符號(Punctuation)\n",
        "import string\n",
        "print('標點符號:', string.punctuation)\n",
        "\n",
        "# 測試文章段落\n",
        "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
        "     \" And yesterday was the best day ever.\"\n",
        "# 讀取停用詞\n",
        "stopword_list = set(nltk.corpus.stopwords.words('english') \n",
        "                    + list(string.punctuation))\n",
        "\n",
        "# 移除停用詞(Removing Stopwords)\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    if is_lower_case:\n",
        "        text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text, filtered_tokens\n",
        "\n",
        "filtered_text, filtered_tokens = remove_stopwords(text) \n",
        "filtered_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "標點符號: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Today great day It even better yesterday And yesterday best day ever'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-u1iUj9Srck"
      },
      "source": [
        "## BOW 測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx1XEvLcSrck",
        "outputId": "9633a04c-766c-435f-8582-e381132fd25c"
      },
      "source": [
        "# 測試文章段落\n",
        "with open('../NLP_data/news.txt','r+', encoding='UTF-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "filtered_text, filtered_tokens = remove_stopwords(text, True) \n",
        "\n",
        "import collections\n",
        "# 生字表的集合\n",
        "word_freqs = collections.Counter()\n",
        "for word in filtered_tokens:\n",
        "    word_freqs[word] += 1\n",
        "print(word_freqs.most_common(20))         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('’', 35), ('stores', 15), ('convenience', 14), ('one', 8), ('—', 8), ('even', 8), ('seoul', 8), ('city', 7), ('korea', 6), ('korean', 6), ('cities', 6), ('people', 5), ('summer', 4), ('new', 4), ('also', 4), ('find', 4), ('store', 4), ('would', 4), ('like', 4), ('average', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocqkJUiSSrcl",
        "outputId": "33b733a3-1312-438f-b1b8-754a53337670"
      },
      "source": [
        "# 移除停用詞(Removing Stopwords)\n",
        "lem = nltk.WordNetLemmatizer()\n",
        "def remove_stopwords_regex(text, is_lower_case=False):\n",
        "    if is_lower_case:\n",
        "        text = text.lower()\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+') # 篩選文數字(Alphanumeric)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [lem.lemmatize(token.strip()) for token in tokens] # 詞形還原\n",
        "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text, filtered_tokens\n",
        "\n",
        "filtered_text, filtered_tokens = remove_stopwords_regex(text, True) \n",
        "word_freqs = collections.Counter()\n",
        "for word in filtered_tokens:\n",
        "    word_freqs[word] += 1\n",
        "print(word_freqs.most_common(20))         "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('store', 19), ('convenience', 14), ('city', 13), ('one', 8), ('even', 8), ('seoul', 8), ('korea', 6), ('korean', 6), ('night', 6), ('food', 5), ('ha', 5), ('people', 5), ('summer', 4), ('new', 4), ('life', 4), ('also', 4), ('find', 4), ('would', 4), ('like', 4), ('chain', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP6vfpJcSrcl",
        "outputId": "e25c93ff-3ae5-4a1c-b9ac-be89fb40b9c7"
      },
      "source": [
        "lem.lemmatize('korean')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'korean'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyoJeJ82Srcm"
      },
      "source": [
        "## 相似詞(Synonyms)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B-9Huq5Srcm",
        "outputId": "dc85c076-72a3-4d3c-d27e-252cf5cd1ada"
      },
      "source": [
        "# 找出相似詞(Synonyms)\n",
        "synonyms = nltk.corpus.wordnet.synsets('love')\n",
        "synonyms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Synset('love.n.01'),\n",
              " Synset('love.n.02'),\n",
              " Synset('beloved.n.01'),\n",
              " Synset('love.n.04'),\n",
              " Synset('love.n.05'),\n",
              " Synset('sexual_love.n.02'),\n",
              " Synset('love.v.01'),\n",
              " Synset('love.v.02'),\n",
              " Synset('love.v.03'),\n",
              " Synset('sleep_together.v.01')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrqzFUYoSrcn",
        "outputId": "5b98dbaa-b5d6-45c3-e695-a7866209ca8e"
      },
      "source": [
        "# 單字說明\n",
        "synonyms[0].definition()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'a strong positive emotion of regard and affection'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZrdEwa2Srcn",
        "outputId": "9f1a4b4f-aaf6-4dd6-9561-83daad1bd891"
      },
      "source": [
        "# 單字的例句\n",
        "synonyms[0].examples()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['his love for his work', 'children need a lot of love']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VG4Nq9nSrcn"
      },
      "source": [
        "## 相反詞(Antonyms)\n",
        "#### 必須呼叫 lemmas 進行詞型還原，再呼叫 antonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvNacKSnSrco",
        "outputId": "9134831d-0e3d-4a2e-c32e-6dfb7e72ecca"
      },
      "source": [
        "# 找出相反詞(Antonyms)\n",
        "antonyms=[]\n",
        "for syn in nltk.corpus.wordnet.synsets('ugly'):\n",
        "    for l in syn.lemmas():\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "antonyms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['beautiful']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jymzku_ySrco"
      },
      "source": [
        "## 詞性標籤(POS Tagging)\n",
        "#### 依照句子結構，顯示每個單字的詞性。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiWxAvd1Srco",
        "outputId": "c9ec30e2-5788-4058-ef0f-e0bc5f137521"
      },
      "source": [
        "# 找出詞性標籤(POS Tagging)\n",
        "text='I am a human being, capable of doing terrible things'\n",
        "sentences=nltk.sent_tokenize(text)\n",
        "for sent in sentences:\n",
        "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('human', 'JJ'), ('being', 'VBG'), (',', ','), ('capable', 'JJ'), ('of', 'IN'), ('doing', 'VBG'), ('terrible', 'JJ'), ('things', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuP0hEp_Srcp"
      },
      "source": [
        "### POS Tagging: https://baike.baidu.com/item/%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8/2783103\n",
        "### https://verbs.colorado.edu/chinese/posguide.3rd.ch.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzG88FWHSrcp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}