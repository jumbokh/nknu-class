{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "12_02_LSTM_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWqzh03Zari4"
      },
      "source": [
        "# 影評資料集(IMDB movie review)情緒分析 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRxYIpBvml2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74191ccc-f4ac-4d7d-e2ae-0efcb5cedecd"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8vEtVKBari7"
      },
      "source": [
        "# 載入相關套件\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pWgsag7zFcr",
        "outputId": "516a3154-9e06-4d34-fb1c-38376afd2e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsJ6HOaSari8"
      },
      "source": [
        "# 參數設定\n",
        "batch_size = 128            # 批量\n",
        "embedding_output_dims = 15  # 嵌入層輸出維度\n",
        "max_sequence_length = 300   # 句子最大字數\n",
        "num_distinct_words = 5000   # 字典\n",
        "number_of_epochs = 5        # 訓練執行週期\n",
        "validation_split = 0.20     # 驗證資料比例\n",
        "verbosity_mode = 1          # 訓練資料訊息顯示程度"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsLrA-Sqari9",
        "outputId": "72181b0e-9ba8-4040-e66d-6a59a5a0ae1d"
      },
      "source": [
        "# 載入 IMDB 影評資料集，TensorFlow 已將資料轉為索引值\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
        "    num_words=num_distinct_words)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# 長度不足時補 0\n",
        "padded_inputs = pad_sequences(x_train, maxlen=max_sequence_length\n",
        "                              , value = 0.0) \n",
        "padded_inputs_test = pad_sequences(x_test, maxlen=max_sequence_length\n",
        "                                   , value = 0.0) \n",
        "\n",
        "# 建立模型\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_distinct_words, embedding_output_dims, \n",
        "                    input_length=max_sequence_length))\n",
        "model.add(LSTM(10))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 指定優化器、損失函數\n",
        "model.compile(optimizer=Adam(), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
        "\n",
        "# 模型彙總資訊\n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "(25000,)\n",
            "(25000,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 300, 15)           75000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 10)                1040      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 76,051\n",
            "Trainable params: 76,051\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb2xyEpSari-",
        "outputId": "7824d1e8-cc2b-445a-b5c2-672e195dae74"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeAR2nMNari_",
        "outputId": "28898381-ca06-4f74-d902-af28d281607f"
      },
      "source": [
        "# 訓練模型\n",
        "history = model.fit(padded_inputs, y_train, batch_size=batch_size, \n",
        "            epochs=number_of_epochs, verbose=verbosity_mode, \n",
        "            validation_split=validation_split)\n",
        "\n",
        "# 模型評估\n",
        "test_results = model.evaluate(padded_inputs_test, y_test, verbose=False)\n",
        "print(f'Loss: {test_results[0]}, Accuracy: {100*test_results[1]}%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "157/157 [==============================] - 16s 53ms/step - loss: 0.6262 - accuracy: 0.6718 - val_loss: 0.4634 - val_accuracy: 0.8242\n",
            "Epoch 2/5\n",
            "157/157 [==============================] - 8s 51ms/step - loss: 0.3904 - accuracy: 0.8487 - val_loss: 0.3632 - val_accuracy: 0.8558\n",
            "Epoch 3/5\n",
            "157/157 [==============================] - 8s 50ms/step - loss: 0.2986 - accuracy: 0.8877 - val_loss: 0.3212 - val_accuracy: 0.8712\n",
            "Epoch 4/5\n",
            "157/157 [==============================] - 8s 51ms/step - loss: 0.2427 - accuracy: 0.9133 - val_loss: 0.3129 - val_accuracy: 0.8718\n",
            "Epoch 5/5\n",
            "157/157 [==============================] - 8s 51ms/step - loss: 0.2101 - accuracy: 0.9285 - val_loss: 0.3160 - val_accuracy: 0.8714\n",
            "Loss: 0.31979528069496155, Accuracy: 87.16800212860107%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7JTjqPJari_"
      },
      "source": [
        "# 模型存檔\n",
        "model.save('LSTM_IMDB.h5')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUMhFTcHarjA",
        "outputId": "c94b8d73-15f0-422b-982a-dd746fcef0d3"
      },
      "source": [
        "# 取得字詞與索引的對照表字典\n",
        "imdb_dict = imdb.get_word_index()\n",
        "list(imdb_dict.keys())[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "1654784/1641221 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fawn',\n",
              " 'tsukino',\n",
              " 'nunnery',\n",
              " 'sonja',\n",
              " 'vani',\n",
              " 'woods',\n",
              " 'spiders',\n",
              " 'hanging',\n",
              " 'woody',\n",
              " 'trawling']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHGTSMiJarjB"
      },
      "source": [
        "# 反轉字典，變成索引與字詞的對照表\n",
        "imdb_dict_reversed = {}\n",
        "for k, v in imdb_dict.items():\n",
        "    imdb_dict_reversed[v] = k"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-qdr0-VarjC",
        "scrolled": true,
        "outputId": "30dd4621-be10-45e0-ee52-dfe3d8d3ff43"
      },
      "source": [
        "# 顯示測試資料前兩筆為文字\n",
        "text = []\n",
        "for i, line in enumerate(padded_inputs_test[:2]):\n",
        "    text.append('')\n",
        "    for j, word in enumerate(line):\n",
        "        if word != 0:\n",
        "            text[i] += imdb_dict_reversed[word]+' '\n",
        "\n",
        "print('\\n\\n\\n'.join(text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the wonder own as by is sequence i i and and to of hollywood br of down and getting boring of ever it sadly sadly sadly i i was then does don't close and after one carry as by are be and all family turn in does as three part in another some to be probably with world and her an have and beginning own as is sequence \n",
            "\n",
            "\n",
            "the as you world's is quite br and most that quest are chase to being quickly of little it time hell to plot br of something long put are of every place this and and of and storytelling being nasty not of you warren in is failed club i i of films pay so sequences and film okay uses to received and if time done for room and viewer as cartoon of gives to forgettable br be because many these of and and contained gives it wreck scene to more was two when had find as you another it of themselves probably who and storytelling if itself by br about 1950's films not would effects that her box to miike for if hero close seek end is very together movie of and got say kong and fred close bore there is playing lot of and pan place trilogy of lacks br of their time much this men as on it is telling program br and okay and to frustration at corner and she of sequences to political clearly in of drugs keep guy i i was throwing room and as it by br be plot many for occasionally film and boyfriend difficult kid as you it failed not if gerard to if woman in and is police fi spooky or of self what have pretty in can so suit you good 2 which why super as it main of my i i  if time screenplay in same this remember and have action one in realistic that better of lessons \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IQlQE0k5arjD",
        "scrolled": true,
        "outputId": "685ab433-3c3f-4cb1-82d5-ace4efa1d81b"
      },
      "source": [
        "imdb_dict_reversed[488]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'close'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJOXq8PImPWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e23cb3-8d20-4830-ece0-8f9b50ca9481"
      },
      "source": [
        "imdb_dict['close']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "488"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw1N_5qLnHzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5be262-352c-4148-f8ce-39b614a2d86e"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOip6zMoarjD"
      },
      "source": [
        "import nltk\n",
        "\n",
        "# 以上述語句測試\n",
        "X_tokens = []\n",
        "for line in text:\n",
        "    tokens = nltk.word_tokenize(line)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    X_tokens.append(tokens)\n",
        "    \n",
        "# 轉為索引值\n",
        "import numpy as np\n",
        "X_index = np.zeros((len(text), max_sequence_length))\n",
        "for i, line in enumerate(X_tokens):\n",
        "    for j, word in enumerate(line):\n",
        "        if j >= max_sequence_length:\n",
        "            break\n",
        "        if word in imdb_dict:\n",
        "            # 因為num_distinct_words=5000, 怕反轉為數字時會出錯，超過5000時設為0\n",
        "            if imdb_dict[word] < num_distinct_words:\n",
        "                X_index[i, j] = imdb_dict[word]\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w0pfewfngGH"
      },
      "source": [
        "def sigmoid(x):\n",
        "    sig = 1 / (1 + math.exp(-x))\n",
        "    return round(sig,2)\n",
        "def mpredit(x):\n",
        "  d = 0\n",
        "  if x>0.5:\n",
        "    d = 1\n",
        "  return d\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0HthQhVmPWs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91df7efa-d56b-44c8-e959-152bbc8338ba"
      },
      "source": [
        "X_index[0, :65].astype(np.int)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,  591,  202,   14,   31,    6,  717,   10,   10,    2,    2,\n",
              "          5,    4,  360,    7,    4,  177,    2,  394,  354,    4,  123,\n",
              "          9, 1035, 1035, 1035,   10,   10,   13,   92,  124,   78,    0,\n",
              "        488,    2,  100,   28, 1668,   14,   31,   23,   27,    2,   29,\n",
              "        220,  468,    8,  124,   14,  286,  170,    8,  157,   46,    5,\n",
              "         27,  239,   16,  179,    2,   38,   32,   25,    2,  451])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVzzeUVPnqb3"
      },
      "source": [
        "# 長度不足時補 0\n",
        "padded_inputs = pad_sequences(X_index, maxlen=max_sequence_length, \n",
        "                      value = 0.0) "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xFV6mHParjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c4bc4c-b0a6-4af9-8264-0f81b43f0215"
      },
      "source": [
        "# 預測\n",
        "#model.predict_classes(padded_inputs)\n",
        "predict_x=model.predict(padded_inputs) \n",
        "classes_x=np.argmax(predict_x,axis=1)\n",
        "print(classes_x)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqRRWO3BarjE"
      },
      "source": [
        "# 以原資料預測，確認答案相同\n",
        "#model.predict_classes(padded_inputs_test[:2])\n",
        "predict_x=model.predict(padded_inputs_test[:2]) \n",
        "classes_x=np.argmax(predict_x,axis=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGKJkjikarjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6003330b-612d-46c7-8637-3ca06f575732"
      },
      "source": [
        "print(classes_x)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPureaxioy4V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}