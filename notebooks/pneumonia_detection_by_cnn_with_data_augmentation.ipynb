{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "pneumonia-detection-by-cnn-with-data-augmentation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumbokh/nknu-class/blob/main/notebooks/pneumonia_detection_by_cnn_with_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "jNUtyRQaD_xP"
      },
      "source": [
        "# **Pneumonia Detection by CNN with Data Augmentation** \n",
        "\n",
        "2020/03/15\n",
        "+ 本範例利用 TensorFlow 2.0 架構下的 `tf.keras` 套件，來建立 CNN 模型，透過胸腔 X-光片影像進行肺炎偵測。\n",
        "+ 資料部份：\n",
        "    - 將 \"Chest_Xray\" 影像資料匯入，並修改、統一其影像尺寸為 (224, 224, 3)。\n",
        "    - 其中，資料匯入區分為 train、test 和 val 三個部分。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGLm5G-VD_xV"
      },
      "source": [
        "---------------------------\n",
        "## CONTENT\n",
        "1. [ APPROACH ](#approach)\n",
        "2. [ Data Preprocessing ](#preprocessing)\n",
        "    + [ Importing Training Datasets ](#TrainData)\n",
        "    + [ Importing Validation Datasets ](#ValData)\n",
        "    + [ Importing Test Datasets ](#TestData)\n",
        "3. [ CNN Model with *tf.keras* ](#CNNModel)\n",
        "    + [ Forward Propagation ](#Forwardpropagation)\n",
        "    + [ Model Summary & Plotting the Model ](#ModelSummary)\n",
        "- [ Start Training - from Coarse to Fine ](#StartTraining)\n",
        "    + [ Setting Hyperparameters ](#SettingHyperparameters)\n",
        "    + [ STAGE 1 - Coarse Training without Data Augmentation ](#Stage1)\n",
        "        + [ Backpropagation for STAGE 1 ](#Backpropagation1)\n",
        "    + [ STAGE 2 - Fine Training *with* Data Augmentation ](#Stage2)\n",
        "        + [ Backpropagation for STAGE 2 ](#Backpropagation2)\n",
        "- [ Saving the Entire Model with HDF5 Format ](#SavingEntireModel)\n",
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubi1LwiaD_xW"
      },
      "source": [
        "<a id='approach'></a>\n",
        "## 1. APPROACH\n",
        "> + 本範例採用 **From Coarse to Fine** 的調校過程 (Tuning Process)：\n",
        "    - 首先，利用原始影像資料(raw images)進行 CNN Model 參數調校，並輸出結果 (參考 < STAGE 1 : Coarse Training without Data Augmentation > 部份)。\n",
        "    - 之後，利用 \"**資料擴增 (Data Augmentation)**\" 技術，在程式執行階段 (runtime) 增加 Training 資料量，再次對 CNN Model 進行參數調校，並輸出結果 (參考 < STAGE 2 > 部份)。\n",
        "> + 其**目的**是在 \"**有效縮短運算時間，減少佔用記憶體空間，同時提升預測結果的準確度**\"，達成整體效能(performance)的提升。\n",
        "* > + 本範例中，**CNN Model** 採用 \"Batch Normalization\"、\"Dropout\"以及 \"Learning-Rate Decay\" 等技術。\n",
        "> + 下列程式同時輸出 < STAGE 1 > 和 < STAGE 2 > 結果做為參考，並輸出、儲存整個預測模型。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "zdQlgCriD_xX"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTw9mDmREf8E"
      },
      "source": [
        "\n",
        "我們要完成的事情\n",
        "到 kaggle 下載  ⇒  可以用 token\n",
        "\n",
        "資料都是在目錄結構，整理成 dataframe\n",
        "\n",
        "⇒  考慮用 data_from_frame\n",
        "\n",
        "⇒  將 str 含與不含 \"mask\" 區隔，放進一個 dataframe 的兩個 columns 裡頭。\n",
        "\n",
        "⇒  如果要只針對 有腫瘤的 mask，可以加一欄標註，當然從 mask matrix 就可以判斷出又沒有腫瘤\n",
        "\n",
        "data_generator 設計\n",
        "\n",
        "UNET 設計  ⇒ \n",
        "\n",
        "可以學習設計 resnet block, unet structure\n",
        "keras.models.Model 的寫法\n",
        "可以研究為什麼要 resnet\n",
        "compile and fit\n",
        "\n",
        "熟悉 matplotlib.pyplot, plt.imshow(plt.imread(filename))\n",
        "\n",
        "further practice:\n",
        "\n",
        "focal error design"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqn4wMYzEr8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDCzTXcJExmL"
      },
      "source": [
        "下載 Kaggle 資料\n",
        "請先登入 Kaggle，在網頁右上角，點選個人帳號圖示，進入設定，然後啟動 “Create New API Token”，將會產生個人 API json 檔同時自動下載到個人電腦。\n",
        "將收到的 json 檔裡頭的 username 與 key 填入下一格中的 api_token 中。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La_L6miuL0Zl"
      },
      "source": [
        "### https://github.com/Kaggle/kaggle-api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMXhXL-UEtUV"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqFwgdX8E3mO"
      },
      "source": [
        "!cp drive/MyDrive/mri_process.py .\n",
        "from mri_process import kaggle_download, process, show_img_mask, DataGenerator, prediction, tversky, tversky_loss, focal_tversky"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QUsMZS3E8uz"
      },
      "source": [
        "api_token = kaggle_download(key = \"86e00cd8ed6c732f7c233d9599e581ee\")\n",
        "!cp drive/MyDrive/kaggle.json /root/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        " \n",
        "os.chdir('/content')\n",
        "!kaggle competitions download -c pneumonia-detection\n",
        "#!kaggle datasets download -d mateuszbuda/lgg-mri-segmentation\n",
        "#!kaggle competitions download -c facial-keypoints-detection\n",
        "!unzip pneumonia-detection.zip \n",
        "clear()\n",
        "del api_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24qGwiNIELj2"
      },
      "source": [
        "!wget -O full_numpy_bitmap_apple.npy https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/apple.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgMn3bOsETth"
      },
      "source": [
        "# Eiffel Tower\n",
        "#!wget -O full_numpy_bitmap_Eiffel.npy https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/The%20Eiffel%20Tower.npy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JHiNH1cpD_xa"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mimg\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from os import listdir, makedirs, getcwd, remove\n",
        "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, SeparableConv2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy2Fb5jMD_xb"
      },
      "source": [
        "<a id='preprocessing'></a>\n",
        "## 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kaGiCd5fD_xc"
      },
      "source": [
        "# Input data files are available in the \"../input/\" directory.\n",
        "INPUT_PATH = \"../input/pneumonia-detection/chest_xray\"\n",
        "\n",
        "# List the files in the input directory.\n",
        "print(os.listdir(INPUT_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLCrjh5ZD_xd"
      },
      "source": [
        "> ### [ File Directory ]\n",
        "       +-- input \n",
        "             |-- pneumonia-detection\n",
        "                  |-- chest_xray\n",
        "                         |-- test\n",
        "                               |-- NORMAL\n",
        "                               |-- PNEUMONIA\n",
        "                         |-- train\n",
        "                               |-- NORMAL\n",
        "                               |-- PNEUMONIA\n",
        "                         |-- val\n",
        "                               |-- NORMAL\n",
        "                               |-- PNEUMONIA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOU69H6bD_xe"
      },
      "source": [
        "### Training Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qxa5ptu-D_xf"
      },
      "source": [
        "# list of all the training images\n",
        "train_normal = Path(INPUT_PATH + '/train/NORMAL').glob('*.jpeg')\n",
        "train_pneumonia = Path(INPUT_PATH + '/train/PNEUMONIA').glob('*.jpeg')\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Train data format in (img_path, label) \n",
        "# Labels for [ the normal cases = 0 ] & [the pneumonia cases = 1]\n",
        "# ---------------------------------------------------------------\n",
        "normal_data = [(image, 0) for image in train_normal]\n",
        "pneumonia_data = [(image, 1) for image in train_pneumonia]\n",
        "\n",
        "train_data = normal_data + pneumonia_data\n",
        "\n",
        "# Get a pandas dataframe from the data we have in our list \n",
        "train_data = pd.DataFrame(train_data, columns=['image', 'label'])\n",
        "\n",
        "# Checking the dataframe...\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qum9MJEED_xf"
      },
      "source": [
        "# Checking the dataframe...\n",
        "train_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6DWYWm9PD_xg"
      },
      "source": [
        "# Shuffle the data \n",
        "train_data = train_data.sample(frac=1., random_state=100).reset_index(drop=True)\n",
        "\n",
        "# Checking the dataframe...\n",
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LXFBUBiSD_xg"
      },
      "source": [
        "print(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zpHKfzsZD_xg"
      },
      "source": [
        "# Counts for both classes\n",
        "count_result = train_data['label'].value_counts()\n",
        "print('Total of Train Data : ', len(train_data), '  (0 : Normal; 1 : Pneumonia)')\n",
        "print(count_result)\n",
        "\n",
        "# Plot the results \n",
        "plt.figure(figsize=(8,5))\n",
        "sns.countplot(x = 'label', data =  train_data)\n",
        "plt.title('Number of classes', fontsize=16)\n",
        "plt.xlabel('Class type', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "plt.xticks(range(len(count_result.index)), \n",
        "           ['Normal : 0', 'Pneumonia : 1'], \n",
        "           fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pQb9BJPJD_xh"
      },
      "source": [
        "fig, ax = plt.subplots(3, 4, figsize=(20,15))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    image = imread(train_data.image[i])\n",
        "    axi.imshow(image, cmap='bone')\n",
        "    axi.set_title(('Normal' if train_data.label[i] == 0 else 'Pneumonia') \n",
        "                  + '  [size=' + str(image.shape) +']',\n",
        "                  fontsize=14)\n",
        "    axi.set(xticks=[], yticks=[])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KJet9mCwD_xh"
      },
      "source": [
        "train_data.to_numpy().shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iE3cY11D_xi"
      },
      "source": [
        "### Import X-ray Image Datasets from /train, /val & /test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K4Z-kHSOD_xi"
      },
      "source": [
        "# ----------------------------------------------------------------------\n",
        "#  Loading X-ray Images datasets from file 3 directories, respectively. \n",
        "# ----------------------------------------------------------------------\n",
        "def load_data(files_dir='/train'):\n",
        "    # list of the paths of all the image files\n",
        "    normal = Path(INPUT_PATH + files_dir + '/NORMAL').glob('*.jpeg')\n",
        "    pneumonia = Path(INPUT_PATH + files_dir + '/PNEUMONIA').glob('*.jpeg')\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    # Data-paths' format in (img_path, label) \n",
        "    # labels : for [ Normal cases = 0 ] & [ Pneumonia cases = 1 ]\n",
        "    # --------------------------------------------------------------\n",
        "    normal_data = [(image, 0) for image in normal]\n",
        "    pneumonia_data = [(image, 1) for image in pneumonia]\n",
        "\n",
        "    image_data = normal_data + pneumonia_data\n",
        "\n",
        "    # Get a pandas dataframe for the data paths \n",
        "    image_data = pd.DataFrame(image_data, columns=['image', 'label'])\n",
        "    \n",
        "    # Shuffle the data \n",
        "    image_data = image_data.sample(frac=1., random_state=100).reset_index(drop=True)\n",
        "    \n",
        "    # Importing both image & label datasets...\n",
        "    x_images, y_labels = ([data_input(image_data.iloc[i][:]) for i in range(len(image_data))], \n",
        "                         [image_data.iloc[i][1] for i in range(len(image_data))])\n",
        "\n",
        "    # Convert the list into numpy arrays\n",
        "    x_images = np.array(x_images)\n",
        "    y_labels = np.array(y_labels)\n",
        "    \n",
        "    print(\"Total number of images: \", x_images.shape)\n",
        "    print(\"Total number of labels: \", y_labels.shape)\n",
        "    \n",
        "    return x_images, y_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cpoBTsH8D_xi"
      },
      "source": [
        "# ---------------------------------------------------------\n",
        "#  1. Resizing all the images to 224x224 with 3 channels.\n",
        "#  2. Then, normalize the pixel values.  \n",
        "# ---------------------------------------------------------\n",
        "def data_input(dataset):\n",
        "    # print(dataset.shape)\n",
        "    for image_file in dataset:\n",
        "        image = cv2.imread(str(image_file))\n",
        "        image = cv2.resize(image, (224,224))\n",
        "        if image.shape[2] == 1:\n",
        "            # np.dstack(): Stack arrays in sequence depth-wise \n",
        "            #              (along third axis).\n",
        "            # https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html\n",
        "            image = np.dstack([image, image, image])\n",
        "        \n",
        "        # ----------------------------------------------------------\n",
        "        # cv2.cvtColor(): The function converts an input image \n",
        "        #                 from one color space to another. \n",
        "        # [Ref.1]: \"cvtColor - OpenCV Documentation\"\n",
        "        #     - https://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html\n",
        "        # [Ref.2]: \"Python计算机视觉编程- 第十章 OpenCV\" \n",
        "        #     - https://yongyuan.name/pcvwithpython/chapter10.html\n",
        "        # ----------------------------------------------------------\n",
        "        x_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Normalization\n",
        "        x_image = x_image.astype(np.float32)/255.\n",
        "        return x_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82kvDBWGD_xj"
      },
      "source": [
        "<a id=\"TrainData\"></a>\n",
        "+ ### Importing Training Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b0EgNX6DD_xj"
      },
      "source": [
        "# Import train dataset...\n",
        "x_train, y_train = load_data(files_dir='/train')\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZWtur5soD_xj"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tea2N52KD_xj"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "r2RdDOJTD_xk"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN-moKzPD_xk"
      },
      "source": [
        "<a id=\"ValData\"></a>\n",
        "+ ### Importing Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ls6O6P9ND_xk"
      },
      "source": [
        "# Import validation dataset...\n",
        "x_val, y_val = load_data(files_dir='/val')\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9C6lA_OZD_xk"
      },
      "source": [
        "y_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItPxe0VeD_xl"
      },
      "source": [
        "<a id=\"TestData\"></a>\n",
        "+ ### Importing Test Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2p7JaAQTD_xl"
      },
      "source": [
        "# Import test dataset...\n",
        "x_test, y_test = load_data(files_dir='/test')\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VCyBeUfND_xl"
      },
      "source": [
        "# Counts for both classes\n",
        "count_result = pd.Series(y_test).value_counts()\n",
        "print('Total of Test Data : ', len(y_test), '  (0 : Normal; 1 : Pneumonia)')\n",
        "print('------------------')\n",
        "print(count_result)\n",
        "print('------------------')\n",
        "print('1 :  ', count_result[1]/sum(count_result))\n",
        "print('0 :  ', count_result[0]/sum(count_result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HNOq9zl_D_xl"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJAmmELdD_xl"
      },
      "source": [
        "<a id='CNNModel'></a>\n",
        "## 3. CNN Model by *tf.keras*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itvPBjbkD_xm"
      },
      "source": [
        "<a id=\"Forwardpropagation\"></a>\n",
        "+ ### Forward Propagation - with Batch Normalization and Dropout\n",
        "    + Conv2D layer\n",
        "    + 1x1 Convolution ([Ref]: Prof Andrew Ng, \"Inception Module\", https://www.youtube.com/watch?v=KfV8CJh7hE0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sMDb1bMZD_xm"
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (5,5), activation='relu', padding='same', \n",
        "           input_shape=(224,224,3), name='Conv1_1'),\n",
        "    BatchNormalization(name='bn1_1'),\n",
        "    Conv2D(32, (5,5), activation='relu', padding='same', name='Conv1_2'),\n",
        "    BatchNormalization(name='bn1_2'),\n",
        "    Conv2D(32, (5,5), activation='relu', padding='same', name='Conv1_3'),\n",
        "    BatchNormalization(name='bn1_3'),\n",
        "    MaxPooling2D((2,2), name='MaxPool1'),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_1'),\n",
        "    BatchNormalization(name='bn2_1'),\n",
        "    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_2'),\n",
        "    BatchNormalization(name='bn2_2'),\n",
        "    Conv2D(48, (3,3), activation='relu', padding='same', name='Conv2_3'),\n",
        "    BatchNormalization(name='bn2_3'),    \n",
        "    MaxPooling2D((2,2), name='MaxPool2'),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_1'),\n",
        "    BatchNormalization(name='bn3_1'),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_2'),\n",
        "    BatchNormalization(name='bn3_2'),\n",
        "    Conv2D(64, (3,3), activation='relu', padding='same', name='Conv3_3'),\n",
        "    BatchNormalization(name='bn3_3'),\n",
        "    MaxPooling2D((2,2), name='MaxPool3'),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    # ----------------------------------------------------------------------\n",
        "    # Using \"1x1 convolution layer\" to lower the complexity of computing\n",
        "    # [Ref]: Prof Andrew Ng, \"Inception Module\", \n",
        "    #        https://www.youtube.com/watch?v=KfV8CJh7hE0\n",
        "    # ----------------------------------------------------------------------\n",
        "    Conv2D(64, (1,1), activation='relu', padding='same', name='Conv4_1_1x1'),\n",
        "    BatchNormalization(name='bn4_1_1x1'),\n",
        "    Conv2D(128, (3,3), activation='relu', padding='same', name='Conv4_2'),\n",
        "    BatchNormalization(name='bn4_2'),\n",
        "    MaxPooling2D((2,2), name='MaxPool4'),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Using \"1x1 convolution layer\" \n",
        "    Conv2D(128, (1,1), activation='relu', padding='same', name='Conv5_1_1x1'),\n",
        "    BatchNormalization(name='bn5_1_1x1'),\n",
        "    Conv2D(256, (3,3), activation='relu', padding='same', name='Conv5_2'),\n",
        "    BatchNormalization(name='bn5_2'),\n",
        "    MaxPooling2D((2,2), name='MaxPool5'),\n",
        "    Dropout(0.25),\n",
        "    \n",
        "    # Using \"1x1 convolution layer\" \n",
        "    Conv2D(256, (1,1), activation='relu', padding='same', name='Conv6_1x1'),\n",
        "    BatchNormalization(name='bn6_1x1'),\n",
        "    Conv2D(512, (3,3), activation='relu', name='Conv6_2'),\n",
        "    BatchNormalization(name='bn6_2'),\n",
        "    Dropout(0.5),\n",
        "    \n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu', name='fc'), \n",
        "    BatchNormalization(name='bn_fc'),\n",
        "    Dropout(0.25),\n",
        "    Dense(1, activation='sigmoid', name='Output') \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvYMRyT4D_xm"
      },
      "source": [
        "<a id=\"ModelSummary\"></a>\n",
        "+ ### Model Summary & Plotting the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_fgzc_SKD_xm"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4RSiSsSD_xn"
      },
      "source": [
        "> + ### 此處可見 CNN Model 有 2,669,361 個參數需要進行訓練、調校！\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bTRAC_nED_xn"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, dpi=85)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWUpfOycD_xn"
      },
      "source": [
        "<a id=\"StartTraining\"></a>\n",
        "## 4. Start Training with Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGvfj6-lD_xn"
      },
      "source": [
        "<a id=\"SettingHyperparameters\"></a>\n",
        "+ ### Setting Hyperparameters for Training Process\n",
        ">+ 由於 validation dataset 只有 16 筆影像資料 (太少了些)，因此，直接將 training datasets (5216 images) 分割出 4200 筆的 training 資料 (80.5% 資料量)，其餘的 1016 張 X-ray 影像資料做為 validation 資料集。\n",
        ">+ **[ Learning-Rate Decay ] : basic learning rate = 0.001**\n",
        "    + 在 STAGE 1 中，設定 basic learning rate 的 1/10 為 decay rate，取 epoch 數為 10 時，每個 epoch 的 GPU 運算時間約為 20 秒。\n",
        "    + 在 STAGE 2 中，設定 learning_rate 為 basic learning rate 的 1/10 (decay rate =  learning_rate/100)，取 epoch 數為 20，每個 epoch 的 GPU 運算時間約為 45 秒。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DWFXtE9UD_xn"
      },
      "source": [
        "batch_size = 16\n",
        "epochs_stage_1 = 10\n",
        "epochs_stage_2 = 20\n",
        "train_data_num = 4200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLxNcMIWD_xo"
      },
      "source": [
        "<a id=\"Stage1\"></a>\n",
        "+ ### STAGE 1 : Coarse Training *without* Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSIIv6XbD_xo"
      },
      "source": [
        "<a id=\"Backpropagation1\"></a>\n",
        "> + ### Backpropagation - *Optimizer*, *Loss Function* & *Accuracy* for < STAGE 1 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hMD-u-PED_xo"
      },
      "source": [
        "# Adam Optimizer with Learning-rate Decay \n",
        "basic_learning_rate = 0.001\n",
        "#opt = Adam(lr=basic_learning_rate, decay=basic_learning_rate/10.)\n",
        "opt = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zbvB51yqD_xo"
      },
      "source": [
        "## data_augmentation = False\n",
        "print('Not using data augmentation.')\n",
        "epochs = epochs_stage_1\n",
        "history_no_data_aug = model.fit(x_train[:train_data_num], y_train[:train_data_num],\n",
        "                               batch_size=batch_size,\n",
        "                               epochs=epochs,\n",
        "                               validation_data=(x_train[train_data_num:], y_train[train_data_num:]),\n",
        "                               # validation_data=(x_val, y_val),\n",
        "                               shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NBN9VFC3D_xo"
      },
      "source": [
        "history_no_data_aug.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OAkPksqD_xp"
      },
      "source": [
        "> ### Validation-Curve Diagrams for STAGE 1 - Training without Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JVdyH20aD_xp"
      },
      "source": [
        "acc = history_no_data_aug.history['accuracy']\n",
        "val_acc = history_no_data_aug.history['val_accuracy']\n",
        "\n",
        "loss = history_no_data_aug.history['loss']\n",
        "val_loss = history_no_data_aug.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(epochs_range)\n",
        "plt.title('Training and Validation Accuracy - without Data Augmentation')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(epochs_range)\n",
        "plt.title('Training and Validation Loss - without Data Augmentation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a9XeK6aD_xp"
      },
      "source": [
        "> ### Evaluation for Test Datasets in < STAGE 1 > "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6QguCIlyD_xp"
      },
      "source": [
        "# Score trained model.\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XSluyHPVD_xp"
      },
      "source": [
        "# Get predictions\n",
        "preds = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c-EApxBnD_xq"
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k7tWY3aAD_xq"
      },
      "source": [
        "y_pred = []\n",
        "for i in range(len(preds)):\n",
        "    if preds[i] > 0.5 : \n",
        "        y_pred.append(1) \n",
        "    else: \n",
        "        y_pred.append(0)\n",
        "        \n",
        "print(' y_pred = ', np.array(y_pred[:10]))\n",
        "print(' y_test = ', y_test[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO9jiy2BD_xq"
      },
      "source": [
        "> ### Confusion Matrix for < STAGE 1 >\n",
        ">                        ---------------------------------\n",
        ">                        |               |               |\n",
        ">                        |     true      |     false     |\n",
        ">           Normal : 0   |   negative    |    positive   |\n",
        ">                        |     (tn)      |      (fp)     |\n",
        ">      true              |               |               |\n",
        ">      value             ---------------------------------\n",
        ">                        |               |               |\n",
        ">                        |    false      |     true      |\n",
        ">         Pneumonia : 1  |   negative    |    positive   |\n",
        ">                        |     (fn)      |      (tp)     |\n",
        ">                        |               |               |\n",
        ">                        ---------------------------------\n",
        ">                            Normal : 0     Pneumonia : 1\n",
        ">\n",
        ">                                 predicted value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "u-W5xrS-D_xq"
      },
      "source": [
        "mat = confusion_matrix(y_test, y_pred)\n",
        "print(mat)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(mat, square=False, annot=True, fmt ='d', cbar=True, annot_kws={\"size\": 16})\n",
        "plt.title('0 : Normal   1 : Pneumonia', fontsize = 20)\n",
        "plt.xticks(fontsize = 16)\n",
        "plt.yticks(fontsize = 16)\n",
        "plt.xlabel('predicted value', fontsize = 20)\n",
        "plt.ylabel('true value', fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi-2XbErD_xq"
      },
      "source": [
        "> ### Calculating *precision*, *recall*, *accuracy*, *F1_score* & *F2_score* for < STAGE 1 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mm340okyD_xr"
      },
      "source": [
        "# Calculate Precision and Recall\n",
        "tn, fp, fn, tp = mat.ravel()\n",
        "print('tn = {}, fp = {}, fn = {}, tp = {} '.format(tn, fp, fn, tp))\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "f1_score = 2. * precision * recall / (precision + recall)\n",
        "f2_score = 5. * precision * recall / (4. * precision + recall)\n",
        "\n",
        "print(\"\\nTest Recall of the model \\t = {:.4f}\".format(recall))\n",
        "print(\"Test Precision of the model \\t = {:.4f}\".format(precision))\n",
        "print(\"Test Accuracy of the model \\t = {:.4f}\".format(accuracy))\n",
        "print(\"\\nTest F1 score of the model \\t = {:.4f}\".format(f1_score))\n",
        "print(\"\\nTest F2 score of the model \\t = {:.4f}\".format(f2_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBxYoO__D_xr"
      },
      "source": [
        "<a id=\"Stage2\"></a>\n",
        "+ ### STAGE 2 : Fine Training *with* Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DBanS3CD_xr"
      },
      "source": [
        "<a id=\"Backpropagation2\"></a>\n",
        "> + ### Backpropagation - *Optimizer*, *Loss Function* & *Accuracy* for < STAGE 2 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Shfp1NoMD_xs"
      },
      "source": [
        "# Adam Optimizer with Learning-rate Decay \n",
        "lr_with_decay = basic_learning_rate / 10.\n",
        "#opt = Adam(lr=lr_with_decay, decay=lr_with_decay/100.)\n",
        "opt = SGD(lr=0.01,momentum=0.0,decay=0.0,nesterov=False)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKSdJEhcD_xs"
      },
      "source": [
        "> ### Data Augmentation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Mctj2yEfD_xs"
      },
      "source": [
        "def data_augm():\n",
        "    print('Using real-time data augmentation.')\n",
        "    # This will do preprocessing and realtime data augmentation:\n",
        "    datagen = ImageDataGenerator(\n",
        "        # randomly shift images horizontally (fraction of total width)\n",
        "        width_shift_range=0.05,\n",
        "        # randomly shift images vertically (fraction of total height)\n",
        "        height_shift_range=0.05,\n",
        "        # rotation_range=20,\n",
        "        horizontal_flip=True,  # Randomly flip inputs horizontally.\n",
        "        # vertical_flip=True,  # Randomly flip inputs vertically.\n",
        "        # zoom_range=[0.95, 1.05] # Range for random zoom\n",
        "    )\n",
        "    return datagen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0c1EBSsD_xs"
      },
      "source": [
        "> ### Training with Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C67TDd0sD_xs"
      },
      "source": [
        "print('With data augmentation.')\n",
        "datagen = data_augm()\n",
        "epochs = epochs_stage_2\n",
        "\n",
        "# Compute quantities required for feature-wise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(x_train[:train_data_num])\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "history_data_aug = model.fit_generator(datagen.flow(x_train[:train_data_num], y_train[:train_data_num], \n",
        "                                                    batch_size=batch_size),\n",
        "                                                    epochs=epochs,\n",
        "                                                    validation_data=(x_train[train_data_num:], y_train[train_data_num:]),\n",
        "                                                    # validation_data=(x_val, y_val),\n",
        "                                                    workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYsO0e3tD_xs"
      },
      "source": [
        "> ### Validation-Curve Diagrams for STAGE 2 - Training *with* Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5WlZXhNYD_xt"
      },
      "source": [
        "acc = history_data_aug.history['accuracy']\n",
        "val_acc = history_data_aug.history['val_accuracy']\n",
        "\n",
        "loss = history_data_aug.history['loss']\n",
        "val_loss = history_data_aug.history['val_loss']\n",
        "\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(epochs_range)\n",
        "plt.title('Training and Validation Accuracy with Data Augmentation')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(epochs_range)\n",
        "plt.title('Training and Validation Loss with Data Augmentation')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5WrSUbqD_xt"
      },
      "source": [
        "> ### Evaluation with Test Dataset for < STAGE 2 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MncKfk--D_xt"
      },
      "source": [
        "# Score trained model.\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YQRznPWwD_xt"
      },
      "source": [
        "# Get predictions\n",
        "preds = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_Id_HsfLD_xt"
      },
      "source": [
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dZLvbqOyD_xt"
      },
      "source": [
        "y_pred = []\n",
        "for i in range(len(preds)):\n",
        "    if preds[i] > 0.5 : \n",
        "        y_pred.append(1) \n",
        "    else: \n",
        "        y_pred.append(0)\n",
        "        \n",
        "print(' y_pred = ', np.array(y_pred[:10]))\n",
        "print(' y_test = ', y_test[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMBYFiCND_xu"
      },
      "source": [
        "> ### Confusion Matrix for < STAGE 2 >\n",
        ">                        ---------------------------------\n",
        ">                        |               |               |\n",
        ">                        |     true      |     false     |\n",
        ">           Normal : 0   |   negative    |    positive   |\n",
        ">                        |     (tn)      |      (fp)     |\n",
        ">      true              |               |               |\n",
        ">      value             ---------------------------------\n",
        ">                        |               |               |\n",
        ">                        |    false      |     true      |\n",
        ">         Pneumonia : 1  |   negative    |    positive   |\n",
        ">                        |     (fn)      |      (tp)     |\n",
        ">                        |               |               |\n",
        ">                        ---------------------------------\n",
        ">                            Normal : 0     Pneumonia : 1\n",
        ">\n",
        ">                                 predicted value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Ng2m0PYD_xu"
      },
      "source": [
        "mat = confusion_matrix(y_test, y_pred)\n",
        "print(mat)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(mat, square=False, annot=True, fmt ='d', cbar=True, annot_kws={\"size\": 16})\n",
        "plt.title('0 : Normal   1 : Pneumonia', fontsize = 20)\n",
        "plt.xticks(fontsize = 16)\n",
        "plt.yticks(fontsize = 16)\n",
        "plt.xlabel('predicted value', fontsize = 20)\n",
        "plt.ylabel('true value', fontsize = 20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0bMrPxID_xu"
      },
      "source": [
        "### Calculating *precision*, *recall*, *accuracy*, *F1_score* & *F2_score* for < STAGE 2 >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2QZynwGaD_xu"
      },
      "source": [
        "# Calculate Precision and Recall\n",
        "tn, fp, fn, tp = mat.ravel()\n",
        "print('tn = {}, fp = {}, fn = {}, tp = {} '.format(tn, fp, fn, tp))\n",
        "\n",
        "precision = tp/(tp+fp)\n",
        "recall = tp/(tp+fn)\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "f1_score = 2. * precision * recall / (precision + recall)\n",
        "f2_score = 5. * precision * recall / (4. * precision + recall)\n",
        "\n",
        "print(\"\\nTest Recall of the model \\t = {:.4f}\".format(recall))\n",
        "print(\"Test Precision of the model \\t = {:.4f}\".format(precision))\n",
        "print(\"Test Accuracy of the model \\t = {:.4f}\".format(accuracy))\n",
        "print(\"\\nTest F1 score of the model \\t = {:.4f}\".format(f1_score))\n",
        "print(\"\\nTest F2 score of the model \\t = {:.4f}\".format(f2_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVqmA9RzD_xu"
      },
      "source": [
        "<a id=\"Result\"></a>\n",
        "## 5. Results\n",
        "> + 由以上結果可知：經過 30 epochs (i.e., Stage 1 + Stage 2) 的訓練，**Recall** 值接近 100%；這代表預測模型的 false-negative 誤判部分 (亦即，將 Pneumonia 誤判成 Normal 的狀況) 將會下降趨近 0。\n",
        "> + 然而， **F1 score** 仍然還有改進的空間；這是因為 false-positive 誤診部分 (亦即，將 Normal 誤判成 Pneumonia 的狀況) 造成 **Precision** 的精確值偏低的緣故。\n",
        "> + 請想想看，如何改進模型或調校預測模型參數 (hyperparameters)，進而能夠使其 **F1 score** 數值上升。  Good luck！\n",
        "\n",
        "以下將 From-Coarse-to-Fine 全程訓練過程的 Learning Curves 繪製如下，做為參考："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "atgqQTc7D_xu"
      },
      "source": [
        "acc_total = history_no_data_aug.history['accuracy'] + history_data_aug.history['accuracy']\n",
        "val_acc_total = history_no_data_aug.history['val_accuracy'] + history_data_aug.history['val_accuracy']\n",
        "\n",
        "loss_total = history_no_data_aug.history['loss'] + history_data_aug.history['loss']\n",
        "val_loss_total = history_no_data_aug.history['val_loss'] + history_data_aug.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_FAnSdipD_xv"
      },
      "source": [
        "initial_epochs = epochs_stage_1\n",
        "total_epochs = epochs_stage_1 + epochs_stage_2\n",
        "epochs_range = range(1, total_epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(epochs_range, acc_total, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc_total, label='Validation Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.xticks(range(1,total_epochs+1,1))\n",
        "plt.plot([initial_epochs,initial_epochs],\n",
        "          plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(epochs_range, loss_total, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss_total, label='Validation Loss')\n",
        "plt.ylim([0, 1])\n",
        "plt.xticks(range(1,total_epochs+1,1))\n",
        "plt.plot([initial_epochs,initial_epochs],\n",
        "         plt.ylim(), label='Start Fine Tuning')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueLyaXDbD_xv"
      },
      "source": [
        "<a id=\"SavingEntireModel\"></a>\n",
        "## 6. Saving the Entire Model with HDF5 Format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qDx6FOpVD_xv"
      },
      "source": [
        "# Saving the entire model to a HDF5 file：\n",
        "# The '.h5' extension is for the HDF5 format.\n",
        "model.save('PD_HDF5_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2WJtpJ7gD_xv"
      },
      "source": [
        "# Reloading the HDF5 model, including its weights and the optimizer.\n",
        "HDF5_model = tf.keras.models.load_model('PD_HDF5_model.h5')\n",
        "\n",
        "# Show the model architecture\n",
        "HDF5_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e0KdmPYeD_xv"
      },
      "source": [
        "# Evaluate the restored HDF5 model\n",
        "loss, acc = HDF5_model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gExOCTn5D_xv"
      },
      "source": [
        "# submission = pd.concat([pd.Series(range(1,(len(pred)+1)),name = \"ImageId\"),preds],axis = 1)\n",
        "data_subm = {'ImageId': pd.Series(range(1,(len(y_pred)+1))), 'Prediction': y_pred}\n",
        "submission = pd.DataFrame(data_subm)\n",
        "submission = submission.applymap(str)\n",
        "\n",
        "submission.to_csv(\"submission.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}